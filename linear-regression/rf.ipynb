{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b0181d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"D:\\spark-3.2.1-bin-hadoop3.2\")\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63f6f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"rf_exmaple\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a32948f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"data/dummy_data.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ae10f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: string (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "199557f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ffba697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'lpep_pickup_datetime',\n",
       " 'lpep_dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'ehail_fee',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'trip_type',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7ed17b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import random\n",
    "# data_modified = data.select(\"*\", (F.col(\"fare_amount\") - F.col(\"total_amount\")).alias(\"weather\"))\n",
    "# data_modified.columns\n",
    "data_modified = data.withColumn(\"weather\", F.round(F.rand() * 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6262c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+-------+-------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|weather|trip_duration|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+-------+-------------+\n",
      "|       2| 2019-12-18 15:52:30|  2019-12-18 15:54:39|                 N|         1|         264|         264|              5|          0.0|        3.5|  0.5|    0.5|      0.01|         0.0|     null|                  0.3|        4.81|           1|        1|                 0.0|    5.0|        129.0|\n",
      "|       2| 2020-01-01 00:45:58|  2020-01-01 00:56:39|                 N|         5|          66|          65|              2|         1.28|       20.0|  0.0|    0.0|      4.06|         0.0|     null|                  0.3|       24.36|           1|        2|                 0.0|    8.0|        641.0|\n",
      "|       2| 2020-01-01 00:41:38|  2020-01-01 00:52:49|                 N|         1|         181|         228|              1|         2.47|       10.5|  0.5|    0.5|      3.54|         0.0|     null|                  0.3|       15.34|           1|        1|                 0.0|    6.0|        671.0|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+-------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.types import DoubleType\n",
    "def getTripDuration(datetime_start, datetime_end):\n",
    "    try:\n",
    "        result = (datetime.strptime(datetime_end, \"%Y-%m-%d %H:%M:%S\") - datetime.strptime(datetime_start, \"%Y-%m-%d %H:%M:%S\")).total_seconds()\n",
    "        return result\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "tripDurationFunction = F.udf(getTripDuration, DoubleType())\n",
    "# datetime_object = datetime.strptime(\"2019-12-18 15:52:30\", \"%Y-%m-%d %H:%M:%S\")\n",
    "# datetime_object2 = datetime.strptime(\"2019-12-18 15:54:39\", \"%Y-%m-%d %H:%M:%S\")\n",
    "# print(f\"This is datetime: {(datetime_object2 - datetime_object).total_seconds()}\")\n",
    "data_modified = data_modified.withColumn(\"trip_duration\", tripDurationFunction(\"lpep_pickup_datetime\", \"lpep_dropoff_datetime\"))\n",
    "data_modified.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c816abee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'lpep_pickup_datetime',\n",
       " 'lpep_dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'ehail_fee',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'trip_type',\n",
       " 'congestion_surcharge',\n",
       " 'weather',\n",
       " 'trip_duration']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_modified.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21c6e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['PULocationID','DOLocationID','passenger_count','trip_distance','fare_amount','extra','mta_tax','tip_amount','tolls_amount','improvement_surcharge', 'total_amount','payment_type','trip_type','congestion_surcharge','weather','trip_duration'], outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "08643041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime='2019-12-18 15:52:30', lpep_dropoff_datetime='2019-12-18 15:54:39', store_and_fwd_flag='N', RatecodeID=1, PULocationID=264, DOLocationID=264, passenger_count=5, trip_distance=0.0, fare_amount=3.5, extra=0.5, mta_tax=0.5, tip_amount=0.01, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=4.81, payment_type=1, trip_type=1, congestion_surcharge=0.0, weather=5.0, trip_duration=129.0, features=DenseVector([264.0, 264.0, 5.0, 0.0, 3.5, 0.5, 0.5, 0.01, 0.0, 0.3, 4.81, 1.0, 1.0, 0.0, 5.0, 129.0]))]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = assembler.transform(data_modified)\n",
    "output.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d34c5f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output.select(\"features\", \"total_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cbcc3d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|            features|total_amount|\n",
      "+--------------------+------------+\n",
      "|[264.0,264.0,5.0,...|        4.81|\n",
      "|[66.0,65.0,2.0,1....|       24.36|\n",
      "|[181.0,228.0,1.0,...|       15.34|\n",
      "|[129.0,263.0,2.0,...|       25.05|\n",
      "|[210.0,150.0,1.0,...|        11.3|\n",
      "|[35.0,39.0,1.0,3....|        14.8|\n",
      "|[25.0,61.0,1.0,2....|        12.3|\n",
      "|[225.0,89.0,1.0,4...|        21.8|\n",
      "|[129.0,129.0,1.0,...|         6.8|\n",
      "|[129.0,83.0,1.0,0...|         6.8|\n",
      "|[82.0,173.0,1.0,1...|        10.8|\n",
      "|[74.0,69.0,1.0,3....|        15.3|\n",
      "|[74.0,41.0,1.0,1....|         7.8|\n",
      "|[41.0,127.0,1.0,5...|        20.3|\n",
      "|[7.0,260.0,1.0,1....|        10.8|\n",
      "|[7.0,7.0,1.0,1.42...|         8.3|\n",
      "|[7.0,133.0,1.0,15...|       53.16|\n",
      "|[134.0,28.0,1.0,1...|         7.8|\n",
      "|[89.0,39.0,1.0,2....|        11.3|\n",
      "|[66.0,65.0,3.0,1....|         7.8|\n",
      "+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "901b3db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd8c05f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      total_amount|\n",
      "+-------+------------------+\n",
      "|  count|                67|\n",
      "|   mean|15.329104477611926|\n",
      "| stddev|10.160538152554969|\n",
      "|    min|               4.8|\n",
      "|    max|             57.92|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "291a4323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      total_amount|\n",
      "+-------+------------------+\n",
      "|  count|                31|\n",
      "|   mean|18.017419354838715|\n",
      "| stddev|15.473059591376218|\n",
      "|    min|               4.8|\n",
      "|    max|              78.1|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad20f71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------------+\n",
      "|            features|total_amount|        prediction|\n",
      "+--------------------+------------+------------------+\n",
      "|[7.0,133.0,1.0,15...|       53.16|44.356249999999996|\n",
      "|[33.0,164.0,5.0,6...|        78.1|35.299249999999994|\n",
      "|[35.0,39.0,1.0,3....|        14.8|17.593082142857146|\n",
      "|[41.0,235.0,1.0,6...|        27.3| 33.83347619047618|\n",
      "|[41.0,238.0,1.0,1...|       11.05|10.233298809523808|\n",
      "|[41.0,239.0,1.0,2...|       13.55|12.933597069597074|\n",
      "|[42.0,42.0,1.0,0....|         6.8| 6.633379329004329|\n",
      "|[59.0,136.0,1.0,3...|        15.0|16.315485714285717|\n",
      "|[66.0,65.0,2.0,1....|       24.36|23.830794444444447|\n",
      "|[66.0,65.0,3.0,1....|         7.8|  8.44434111721612|\n",
      "|[66.0,66.0,1.0,0....|         4.8| 5.209100396825396|\n",
      "|[74.0,41.0,1.0,1....|         7.8| 8.476886904761907|\n",
      "|[74.0,69.0,1.0,3....|        15.3|16.068969047619053|\n",
      "|[74.0,141.0,1.0,1...|       11.05|10.233563095238095|\n",
      "|[82.0,92.0,2.0,2....|        11.3|11.116642740592741|\n",
      "|[95.0,131.0,1.0,3...|        13.3| 13.26107705627706|\n",
      "|[116.0,152.0,1.0,...|         5.3| 5.513600396825397|\n",
      "|[116.0,152.0,1.0,...|        14.8|16.163665476190477|\n",
      "|[134.0,28.0,1.0,1...|         7.8| 7.687966666666668|\n",
      "|[159.0,78.0,1.0,2...|        13.8|14.198490151515154|\n",
      "|[166.0,236.0,2.0,...|       20.05|18.289864285714287|\n",
      "|[166.0,239.0,2.0,...|        16.3|14.727445238095243|\n",
      "|[181.0,4.0,1.0,5....|       34.26| 39.39716666666665|\n",
      "|[181.0,61.0,1.0,1...|       11.76|11.868678354978359|\n",
      "|[181.0,89.0,1.0,2...|       16.62|15.027115151515158|\n",
      "+--------------------+------------+------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"total_amount\")\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "predictions = rf_model.transform(test_data)\n",
    "predictions.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc247ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f5338241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "import numpy as np\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [int(x) for x in np.linspace(start = 10, stop = 50, num = 3)]) \\\n",
    "    .addGrid(rf.maxDepth, [int(x) for x in np.linspace(start = 5, stop = 25, num = 3)]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c79259b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4513aa13",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "PULocationID does not exist. Available: features, total_amount, CrossValidator_70b04dd63169_rand",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32md:\\DIR1\\Projects\\taxi-fare-prediction\\linear-regression\\rf.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DIR1/Projects/taxi-fare-prediction/linear-regression/rf.ipynb#ch0000020?line=0'>1</a>\u001b[0m cvModel \u001b[39m=\u001b[39m crossval\u001b[39m.\u001b[39;49mfit(train_data)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DIR1/Projects/taxi-fare-prediction/linear-regression/rf.ipynb#ch0000020?line=1'>2</a>\u001b[0m cvPredictions \u001b[39m=\u001b[39m cvModel\u001b[39m.\u001b[39mtransform(test_data)\n",
      "File \u001b[1;32mD:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\ml\\base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=158'>159</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=159'>160</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=160'>161</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=161'>162</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=162'>163</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=163'>164</a>\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[1;32mD:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\ml\\tuning.py:689\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=683'>684</a>\u001b[0m train \u001b[39m=\u001b[39m datasets[i][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcache()\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=685'>686</a>\u001b[0m tasks \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=686'>687</a>\u001b[0m     inheritable_thread_target,\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=687'>688</a>\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam))\n\u001b[1;32m--> <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=688'>689</a>\u001b[0m \u001b[39mfor\u001b[39;00m j, metric, subModel \u001b[39min\u001b[39;00m pool\u001b[39m.\u001b[39mimap_unordered(\u001b[39mlambda\u001b[39;00m f: f(), tasks):\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=689'>690</a>\u001b[0m     metrics[j] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (metric \u001b[39m/\u001b[39m nFolds)\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=690'>691</a>\u001b[0m     \u001b[39mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[1;32mC:\\Python39\\lib\\multiprocessing\\pool.py:870\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/multiprocessing/pool.py?line=867'>868</a>\u001b[0m \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/multiprocessing/pool.py?line=868'>869</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n\u001b[1;32m--> <a href='file:///c%3A/Python39/lib/multiprocessing/pool.py?line=869'>870</a>\u001b[0m \u001b[39mraise\u001b[39;00m value\n",
      "File \u001b[1;32mC:\\Python39\\lib\\multiprocessing\\pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/multiprocessing/pool.py?line=122'>123</a>\u001b[0m job, i, func, args, kwds \u001b[39m=\u001b[39m task\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/multiprocessing/pool.py?line=123'>124</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Python39/lib/multiprocessing/pool.py?line=124'>125</a>\u001b[0m     result \u001b[39m=\u001b[39m (\u001b[39mTrue\u001b[39;00m, func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/multiprocessing/pool.py?line=125'>126</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/multiprocessing/pool.py?line=126'>127</a>\u001b[0m     \u001b[39mif\u001b[39;00m wrap_exception \u001b[39mand\u001b[39;00m func \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[1;32mD:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\ml\\tuning.py:689\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=683'>684</a>\u001b[0m train \u001b[39m=\u001b[39m datasets[i][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcache()\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=685'>686</a>\u001b[0m tasks \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=686'>687</a>\u001b[0m     inheritable_thread_target,\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=687'>688</a>\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam))\n\u001b[1;32m--> <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=688'>689</a>\u001b[0m \u001b[39mfor\u001b[39;00m j, metric, subModel \u001b[39min\u001b[39;00m pool\u001b[39m.\u001b[39mimap_unordered(\u001b[39mlambda\u001b[39;00m f: f(), tasks):\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=689'>690</a>\u001b[0m     metrics[j] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (metric \u001b[39m/\u001b[39m nFolds)\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=690'>691</a>\u001b[0m     \u001b[39mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[1;32mD:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\util.py:326\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/util.py?line=322'>323</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/util.py?line=323'>324</a>\u001b[0m     \u001b[39m# Set local properties in child thread.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/util.py?line=324'>325</a>\u001b[0m     SparkContext\u001b[39m.\u001b[39m_active_spark_context\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39msetLocalProperties(properties)\n\u001b[1;32m--> <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/util.py?line=325'>326</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/util.py?line=326'>327</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/util.py?line=327'>328</a>\u001b[0m     InheritableThread\u001b[39m.\u001b[39m_clean_py4j_conn_for_current_thread()\n",
      "File \u001b[1;32mD:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\ml\\tuning.py:69\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=67'>68</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msingleTask\u001b[39m():\n\u001b[1;32m---> <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=68'>69</a>\u001b[0m     index, model \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(modelIter)\n\u001b[0;32m     <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=69'>70</a>\u001b[0m     \u001b[39m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=70'>71</a>\u001b[0m     \u001b[39m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=71'>72</a>\u001b[0m     \u001b[39m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=72'>73</a>\u001b[0m     \u001b[39m#  all nested stages and evaluators\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/tuning.py?line=73'>74</a>\u001b[0m     metric \u001b[39m=\u001b[39m eva\u001b[39m.\u001b[39mevaluate(model\u001b[39m.\u001b[39mtransform(validation, epm[index]))\n",
      "File \u001b[1;32mD:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\ml\\base.py:69\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=66'>67</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo models remaining.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=67'>68</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcounter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=68'>69</a>\u001b[0m \u001b[39mreturn\u001b[39;00m index, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfitSingleModel(index)\n",
      "File \u001b[1;32mD:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\ml\\base.py:126\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=124'>125</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfitSingleModel\u001b[39m(index):\n\u001b[1;32m--> <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=125'>126</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator\u001b[39m.\u001b[39;49mfit(dataset, paramMaps[index])\n",
      "File \u001b[1;32mD:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\ml\\base.py:159\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=156'>157</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(params, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=157'>158</a>\u001b[0m     \u001b[39mif\u001b[39;00m params:\n\u001b[1;32m--> <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=158'>159</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy(params)\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=159'>160</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=160'>161</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(dataset)\n",
      "File \u001b[1;32mD:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\ml\\pipeline.py:112\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/pipeline.py?line=109'>110</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(stage, Transformer):\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/pipeline.py?line=110'>111</a>\u001b[0m     transformers\u001b[39m.\u001b[39mappend(stage)\n\u001b[1;32m--> <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/pipeline.py?line=111'>112</a>\u001b[0m     dataset \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39;49mtransform(dataset)\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/pipeline.py?line=112'>113</a>\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# must be an Estimator\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/pipeline.py?line=113'>114</a>\u001b[0m     model \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39mfit(dataset)\n",
      "File \u001b[1;32mD:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\ml\\base.py:217\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=214'>215</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_transform(dataset)\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=215'>216</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=216'>217</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(dataset)\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=217'>218</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/base.py?line=218'>219</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be a param map but got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[1;32mD:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\ml\\wrapper.py:350\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/wrapper.py?line=347'>348</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform\u001b[39m(\u001b[39mself\u001b[39m, dataset):\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/wrapper.py?line=348'>349</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/ml/wrapper.py?line=349'>350</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mtransform(dataset\u001b[39m.\u001b[39;49m_jdf), dataset\u001b[39m.\u001b[39msql_ctx)\n",
      "File \u001b[1;32mD:\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1314'>1315</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1315'>1316</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1316'>1317</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1317'>1318</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1319'>1320</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1320'>1321</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1321'>1322</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1323'>1324</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1324'>1325</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mD:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py?line=113'>114</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py?line=114'>115</a>\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py?line=115'>116</a>\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py?line=116'>117</a>\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py?line=117'>118</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py?line=118'>119</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: PULocationID does not exist. Available: features, total_amount, CrossValidator_70b04dd63169_rand"
     ]
    }
   ],
   "source": [
    "cvModel = crossval.fit(train_data)\n",
    "cvPredictions = cvModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f486e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cvPredictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\DIR1\\Projects\\taxi-fare-prediction\\linear-regression\\rf.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DIR1/Projects/taxi-fare-prediction/linear-regression/rf.ipynb#ch0000021?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DIR1/Projects/taxi-fare-prediction/linear-regression/rf.ipynb#ch0000021?line=1'>2</a>\u001b[0m evaluator \u001b[39m=\u001b[39m RegressionEvaluator(labelCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtotal_amount\u001b[39m\u001b[39m\"\u001b[39m, predictionCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m, metricName\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrmse\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DIR1/Projects/taxi-fare-prediction/linear-regression/rf.ipynb#ch0000021?line=2'>3</a>\u001b[0m rmse \u001b[39m=\u001b[39m evaluator\u001b[39m.\u001b[39mevaluate(cvPredictions)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DIR1/Projects/taxi-fare-prediction/linear-regression/rf.ipynb#ch0000021?line=3'>4</a>\u001b[0m rfPred \u001b[39m=\u001b[39m cvModel\u001b[39m.\u001b[39mtransform(df)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DIR1/Projects/taxi-fare-prediction/linear-regression/rf.ipynb#ch0000021?line=4'>5</a>\u001b[0m rfResult \u001b[39m=\u001b[39m rfPred\u001b[39m.\u001b[39mtoPandas()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cvPredictions' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "evaluator = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(cvPredictions)\n",
    "rfPred = cvModel.transform(df)\n",
    "rfResult = rfPred.toPandas()\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ae79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('numTrees - ', bestModel.getNumTrees)\n",
    "print('maxDepth - ', bestModel.getOrDefault('maxDepth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  1.1156543055488335|\n",
      "|2.856515024518558E-7|\n",
      "|1.020307571764078...|\n",
      "|  -0.572135291787589|\n",
      "|1.181408038064546...|\n",
      "|1.204721709058276...|\n",
      "|1.097926105586566...|\n",
      "|-3.00240863282397...|\n",
      "| 9.72314833092014E-8|\n",
      "|1.293392513090907...|\n",
      "|-2.66408530791295...|\n",
      "|5.384456081003463...|\n",
      "| -0.7437696236711648|\n",
      "| -0.5721315556215458|\n",
      "|4.242300040147029E-7|\n",
      "|7.688969994035233E-8|\n",
      "|2.537640142463715E-8|\n",
      "|3.612127308372237E-8|\n",
      "| -0.2860673466072967|\n",
      "|-2.93007900609154...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed4561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3319012336608198"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d40708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995492236652422"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f77e68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      total_amount|\n",
      "+-------+------------------+\n",
      "|  count|                98|\n",
      "|   mean|16.179489795918347|\n",
      "| stddev|12.077604508545692|\n",
      "|    min|               4.8|\n",
      "|    max|              78.1|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.describe().show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
